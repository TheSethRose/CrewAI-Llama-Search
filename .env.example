# LLM Provider (Required) - Set to either 'ollama' or 'openai'
LLM_PROVIDER=ollama

# Ollama Configuration (Required if using Ollama)
OLLAMA_MODEL_NAME=llama3.1
OLLAMA_BASE_URL=http://localhost:11434

# OpenAI Configuration (Required if using OpenAI)
OPENAI_API_KEY=your-api-key-here
OPENAI_MODEL_NAME=gpt-4o-mini

# Crew Settings (Optional)
CREW_VERBOSE=true          # Enable detailed logging output
CREW_MAX_LOOPS=3          # Maximum number of execution loops
CREW_CACHE_DIR=./cache    # Directory for caching responses
CREW_TIMEOUT=300          # Timeout in seconds for operations

# Instructions:
# 1. Copy this file to .env
# 2. Set LLM_PROVIDER to either 'ollama' or 'openai'
# 3. Configure the settings for your chosen provider
# 4. Adjust crew settings as needed
