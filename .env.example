# LLM Configuration (Required)
OLLAMA_MODEL_NAME=ollama/llama3.3
OLLAMA_BASE_URL=http://localhost:11434

# Crew Settings (Optional)
CREW_VERBOSE=true          # Enable detailed logging output
CREW_MAX_LOOPS=3          # Maximum number of execution loops
CREW_CACHE_DIR=./cache    # Directory for caching responses
CREW_TIMEOUT=300          # Timeout in seconds for operations

# Instructions:
# 1. Copy this file to .env
# 2. Update OLLAMA_BASE_URL if using a remote endpoint
# 3. Adjust CREW settings as needed for your use case
